{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f69fbc",
   "metadata": {},
   "source": [
    "## 1. Setup e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb07d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 15:51:58.741808: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-17 15:51:58.785992: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Directorio temporal: /tmp/tf_data_files\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Crear directorio temporal para ejemplos\n",
    "TEMP_DIR = \"/tmp/tf_data_files\"\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "print(f\"Directorio temporal: {TEMP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86791c4f",
   "metadata": {},
   "source": [
    "## 2. Datasets en Memoria: from_tensors vs from_tensor_slices\n",
    "\n",
    "### 2.1 Diferencia fundamental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec73a365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos originales:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "=== from_tensors() ===\n",
      "Número de elementos: 1\n",
      "Elemento shape: (3, 2)\n",
      "Contenido:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "=== from_tensor_slices() ===\n",
      "Número de elementos: 3\n",
      "Elemento 0: shape=(2,), valor=[1 2]\n",
      "Elemento 1: shape=(2,), valor=[3 4]\n",
      "Elemento 2: shape=(2,), valor=[5 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 15:52:01.102675: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Crear datos de ejemplo\n",
    "data = tf.constant([[1, 2], [3, 4], [5, 6]])\n",
    "print(f\"Datos originales:\\n{data}\\n\")\n",
    "\n",
    "# Opción 1: from_tensors - TODO el tensor como UN SOLO elemento\n",
    "ds1 = tf.data.Dataset.from_tensors(data)\n",
    "print(\"=== from_tensors() ===\")\n",
    "print(f\"Número de elementos: {len(list(ds1))}\")\n",
    "for element in ds1:\n",
    "    print(f\"Elemento shape: {element.shape}\")\n",
    "    print(f\"Contenido:\\n{element.numpy()}\\n\")\n",
    "\n",
    "# Opción 2: from_tensor_slices - Cada FILA es un elemento\n",
    "ds2 = tf.data.Dataset.from_tensor_slices(data)\n",
    "print(\"=== from_tensor_slices() ===\")\n",
    "print(f\"Número de elementos: {len(list(ds2))}\")\n",
    "for i, element in enumerate(ds2):\n",
    "    print(f\"Elemento {i}: shape={element.shape}, valor={element.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f83139",
   "metadata": {},
   "source": [
    "### 2.2 Caso de uso típico: Features y Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a23bf6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (4, 2)\n",
      "Labels shape: (4,)\n",
      "\n",
      "Dataset creado con pares (feature, label):\n",
      "  Feature: [1. 2.], Label: 0\n",
      "  Feature: [3. 4.], Label: 1\n",
      "  Feature: [5. 6.], Label: 0\n",
      "  Feature: [7. 8.], Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Datos simulados: features y labels\n",
    "features = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [7.0, 8.0]])\n",
    "labels = np.array([0, 1, 0, 1])\n",
    "\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\\n\")\n",
    "\n",
    "# Crear dataset con pares (feature, label)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "print(\"Dataset creado con pares (feature, label):\")\n",
    "for x, y in dataset:\n",
    "    print(f\"  Feature: {x.numpy()}, Label: {y.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ccf10",
   "metadata": {},
   "source": [
    "## 3. Trabajar con Archivos CSV usando TextLineDataset\n",
    "\n",
    "### 3.1 Crear archivos CSV de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2962f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV creado: /tmp/tf_data_files/data.csv\n",
      "\n",
      "Primeras 5 líneas:\n",
      "  feature1,feature2,feature3,label\n",
      "  0.8407,0.2453,0.7529,0\n",
      "  0.0266,0.4850,0.4117,0\n",
      "  0.6381,0.4080,0.7292,0\n",
      "  0.7227,0.4348,0.8482,0\n"
     ]
    }
   ],
   "source": [
    "# Crear un archivo CSV simple\n",
    "csv_file = os.path.join(TEMP_DIR, \"data.csv\")\n",
    "\n",
    "with open(csv_file, 'w') as f:\n",
    "    f.write(\"feature1,feature2,feature3,label\\n\")  # Header\n",
    "    for i in range(100):\n",
    "        f1 = np.random.rand()\n",
    "        f2 = np.random.rand()\n",
    "        f3 = np.random.rand()\n",
    "        label = np.random.randint(0, 2)\n",
    "        f.write(f\"{f1:.4f},{f2:.4f},{f3:.4f},{label}\\n\")\n",
    "\n",
    "print(f\"Archivo CSV creado: {csv_file}\")\n",
    "print(f\"\\nPrimeras 5 líneas:\")\n",
    "with open(csv_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:\n",
    "            print(f\"  {line.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c207d",
   "metadata": {},
   "source": [
    "### 3.2 Leer CSV con TextLineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e6082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset desde archivo CSV:\n",
      "Tipo: <TextLineDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n",
      "\n",
      "Primeras 3 líneas (sin procesar):\n",
      "  0: feature1,feature2,feature3,label\n",
      "  1: 0.8407,0.2453,0.7529,0\n",
      "  2: 0.0266,0.4850,0.4117,0\n"
     ]
    }
   ],
   "source": [
    "# Leer archivo línea por línea\n",
    "dataset = tf.data.TextLineDataset(csv_file)\n",
    "\n",
    "print(\"Dataset desde archivo CSV:\")\n",
    "print(f\"Tipo: {dataset}\\n\")\n",
    "\n",
    "# Ver primeras líneas (raw)\n",
    "print(\"Primeras 3 líneas (sin procesar):\")\n",
    "for i, line in enumerate(dataset.take(3)):\n",
    "    print(f\"  {i}: {line.numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2f2b3",
   "metadata": {},
   "source": [
    "### 3.3 Parsear CSV con map()\n",
    "\n",
    "Transformamos cada línea de texto en tensores numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96d63ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parseado (features, label):\n",
      "  Ejemplo 0: features=[0.8407 0.2453 0.7529], label=0\n",
      "  Ejemplo 1: features=[0.0266 0.485  0.4117], label=0\n",
      "  Ejemplo 2: features=[0.6381 0.408  0.7292], label=0\n",
      "  Ejemplo 3: features=[0.7227 0.4348 0.8482], label=0\n",
      "  Ejemplo 4: features=[0.9876 0.406  0.3044], label=1\n"
     ]
    }
   ],
   "source": [
    "def parse_csv_line(line):\n",
    "    \"\"\"\n",
    "    Parsea una línea CSV y retorna (features, label)\n",
    "    \"\"\"\n",
    "    # Definir tipos de datos para cada columna\n",
    "    record_defaults = [0.0, 0.0, 0.0, 0]  # 3 floats + 1 int\n",
    "    \n",
    "    # Decodificar CSV\n",
    "    parsed = tf.io.decode_csv(line, record_defaults=record_defaults)\n",
    "    \n",
    "    # Separar features y label\n",
    "    features = tf.stack(parsed[:-1])  # Primeras 3 columnas\n",
    "    label = parsed[-1]                 # Última columna\n",
    "    \n",
    "    return features, label\n",
    "\n",
    "# Saltar header y parsear\n",
    "dataset_parsed = dataset.skip(1).map(parse_csv_line)\n",
    "\n",
    "print(\"Dataset parseado (features, label):\")\n",
    "for i, (features, label) in enumerate(dataset_parsed.take(5)):\n",
    "    print(f\"  Ejemplo {i}: features={features.numpy()}, label={label.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471d695",
   "metadata": {},
   "source": [
    "### 3.4 Pipeline completa para entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "892e3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline de training creada\n",
      "Dataset: <PrefetchDataset element_spec=(TensorSpec(shape=(None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "\n",
      "Batch de features shape: (16, 3)\n",
      "Batch de labels shape: (16,)\n",
      "\n",
      "Primeros 3 ejemplos del batch:\n",
      "  0: features=[0.4411 0.3412 0.0292], label=0\n",
      "  1: features=[0.2682 0.9693 0.317 ], label=0\n",
      "  2: features=[0.7472 0.2661 0.9841], label=1\n"
     ]
    }
   ],
   "source": [
    "def create_csv_dataset(filepath, batch_size=32, shuffle_buffer=1000, is_training=True):\n",
    "    \"\"\"\n",
    "    Crea un dataset optimizado desde un archivo CSV\n",
    "    \n",
    "    - shuffle: SOLO para training (importante!)\n",
    "    - batch: agrupa ejemplos\n",
    "    - prefetch: optimiza rendimiento\n",
    "    \"\"\"\n",
    "    dataset = tf.data.TextLineDataset(filepath)\n",
    "    dataset = dataset.skip(1)  # Saltar header\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Shuffle SOLO en training\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = create_csv_dataset(csv_file, batch_size=16, is_training=True)\n",
    "val_dataset = create_csv_dataset(csv_file, batch_size=16, is_training=False)\n",
    "\n",
    "print(\"Pipeline de training creada\")\n",
    "print(f\"Dataset: {train_dataset}\\n\")\n",
    "\n",
    "# Ver un batch\n",
    "for features_batch, labels_batch in train_dataset.take(1):\n",
    "    print(f\"Batch de features shape: {features_batch.shape}\")\n",
    "    print(f\"Batch de labels shape: {labels_batch.shape}\")\n",
    "    print(f\"\\nPrimeros 3 ejemplos del batch:\")\n",
    "    for i in range(3):\n",
    "        print(f\"  {i}: features={features_batch[i].numpy()}, label={labels_batch[i].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c97b99",
   "metadata": {},
   "source": [
    "## 4. Datasets Sharded (Múltiples Archivos)\n",
    "\n",
    "### 4.1 Crear múltiples archivos CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377a0085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos sharded creados:\n",
      "  /tmp/tf_data_files/data-0000.csv\n",
      "  /tmp/tf_data_files/data-0001.csv\n",
      "  /tmp/tf_data_files/data-0002.csv\n",
      "  /tmp/tf_data_files/data-0003.csv\n",
      "  /tmp/tf_data_files/data-0004.csv\n",
      "\n",
      "Total samples: 1000\n"
     ]
    }
   ],
   "source": [
    "# Crear 5 archivos sharded\n",
    "num_shards = 5\n",
    "samples_per_shard = 200\n",
    "\n",
    "shard_files = []\n",
    "for shard_idx in range(num_shards):\n",
    "    filename = os.path.join(TEMP_DIR, f\"data-{shard_idx:04d}.csv\")\n",
    "    shard_files.append(filename)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"feature1,feature2,feature3,label\\n\")\n",
    "        for i in range(samples_per_shard):\n",
    "            f1 = np.random.rand()\n",
    "            f2 = np.random.rand()\n",
    "            f3 = np.random.rand()\n",
    "            label = np.random.randint(0, 2)\n",
    "            f.write(f\"{f1:.4f},{f2:.4f},{f3:.4f},{label}\\n\")\n",
    "\n",
    "print(f\"Archivos sharded creados:\")\n",
    "for f in shard_files:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "print(f\"\\nTotal samples: {num_shards * samples_per_shard}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e9ff5",
   "metadata": {},
   "source": [
    "### 4.2 Método 1: Leer con list_files y flat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff07ff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados:\n",
      "  0: /tmp/tf_data_files/data-0000.csv\n",
      "  1: /tmp/tf_data_files/data-0001.csv\n",
      "  2: /tmp/tf_data_files/data-0002.csv\n",
      "  3: /tmp/tf_data_files/data-0003.csv\n",
      "  4: /tmp/tf_data_files/data-0004.csv\n"
     ]
    }
   ],
   "source": [
    "# Listar archivos usando patrón\n",
    "file_pattern = os.path.join(TEMP_DIR, \"data-*.csv\")\n",
    "files_dataset = tf.data.Dataset.list_files(file_pattern, shuffle=False)\n",
    "\n",
    "print(\"Archivos encontrados:\")\n",
    "for i, filepath in enumerate(files_dataset):\n",
    "    print(f\"  {i}: {filepath.numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725233a",
   "metadata": {},
   "source": [
    "### 4.3 flat_map: De archivos a líneas\n",
    "\n",
    "**Diferencia clave:**\n",
    "- `map()`: 1 entrada → 1 salida (archivo → archivo procesado)\n",
    "- `flat_map()`: 1 entrada → MUCHAS salidas (1 archivo → muchas líneas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40fbb6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de líneas desde múltiples archivos:\n",
      "Tipo: <FlatMapDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n",
      "\n",
      "Total de líneas: 1000\n",
      "\n",
      "Primeras 5 líneas:\n",
      "  0: 0.9327,0.1193,0.6504,0\n",
      "  1: 0.7062,0.6836,0.9363,1\n",
      "  2: 0.3119,0.6076,0.6110,0\n",
      "  3: 0.9029,0.6125,0.0891,0\n",
      "  4: 0.2686,0.3941,0.6769,1\n"
     ]
    }
   ],
   "source": [
    "def read_csv_file(filepath):\n",
    "    \"\"\"\n",
    "    Lee un archivo CSV y retorna un dataset de líneas\n",
    "    \"\"\"\n",
    "    dataset = tf.data.TextLineDataset(filepath)\n",
    "    dataset = dataset.skip(1)  # Saltar header\n",
    "    return dataset\n",
    "\n",
    "# Usar flat_map para expandir archivos a líneas\n",
    "lines_dataset = files_dataset.flat_map(read_csv_file)\n",
    "\n",
    "print(f\"Dataset de líneas desde múltiples archivos:\")\n",
    "print(f\"Tipo: {lines_dataset}\\n\")\n",
    "\n",
    "# Contar total de líneas\n",
    "total_lines = sum(1 for _ in lines_dataset)\n",
    "print(f\"Total de líneas: {total_lines}\")\n",
    "\n",
    "# Ver ejemplos\n",
    "print(f\"\\nPrimeras 5 líneas:\")\n",
    "for i, line in enumerate(lines_dataset.take(5)):\n",
    "    print(f\"  {i}: {line.numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6bb66b",
   "metadata": {},
   "source": [
    "### 4.4 Pipeline completa para sharded datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "007d12e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sharded creado:\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "\n",
      "Batch shape: (32, 3)\n",
      "Labels shape: (32,)\n",
      "Primeras 5 labels: [0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "def create_sharded_dataset(file_pattern, batch_size=32, is_training=True):\n",
    "    \"\"\"\n",
    "    Crea dataset desde múltiples archivos sharded\n",
    "    \n",
    "    Pipeline: list_files → flat_map → map(parse) → shuffle → batch → prefetch\n",
    "    \"\"\"\n",
    "    # 1. Listar archivos\n",
    "    files = tf.data.Dataset.list_files(file_pattern, shuffle=is_training)\n",
    "    \n",
    "    # 2. Leer todos los archivos (flat_map: archivo → muchas líneas)\n",
    "    dataset = files.flat_map(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1)\n",
    "    )\n",
    "    \n",
    "    # 3. Parsear líneas (map: línea → (features, label))\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # 4. Shuffle solo en training\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    \n",
    "    # 5. Batch y prefetch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Crear dataset desde archivos sharded\n",
    "sharded_train_dataset = create_sharded_dataset(\n",
    "    file_pattern, \n",
    "    batch_size=32, \n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "print(\"Dataset sharded creado:\")\n",
    "print(f\"{sharded_train_dataset}\\n\")\n",
    "\n",
    "# Ver un batch\n",
    "for features_batch, labels_batch in sharded_train_dataset.take(1):\n",
    "    print(f\"Batch shape: {features_batch.shape}\")\n",
    "    print(f\"Labels shape: {labels_batch.shape}\")\n",
    "    print(f\"Primeras 5 labels: {labels_batch.numpy()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2915488",
   "metadata": {},
   "source": [
    "## 5. Comparación: map() vs flat_map()\n",
    "\n",
    "Visualicemos la diferencia con un ejemplo simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0af54f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== map() - duplica cada número ===\n",
      "  2\n",
      "  4\n",
      "  6\n",
      "\n",
      "=== flat_map() - crea rango desde 0 hasta el número ===\n",
      "  0\n",
      "  0\n",
      "  1\n",
      "  0\n",
      "  1\n",
      "  2\n",
      "\n",
      "Observa: flat_map expande 1 elemento en MÚLTIPLES elementos\n"
     ]
    }
   ],
   "source": [
    "# Dataset de ejemplo: números\n",
    "numbers = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "\n",
    "# map(): 1 entrada → 1 salida\n",
    "print(\"=== map() - duplica cada número ===\")\n",
    "mapped = numbers.map(lambda x: x * 2)\n",
    "for val in mapped:\n",
    "    print(f\"  {val.numpy()}\")\n",
    "\n",
    "# flat_map(): 1 entrada → MUCHAS salidas\n",
    "print(\"\\n=== flat_map() - crea rango desde 0 hasta el número ===\")\n",
    "# Convertir a int64 para evitar error de tipo\n",
    "flat_mapped = numbers.flat_map(lambda x: tf.data.Dataset.range(tf.cast(x, tf.int64)))\n",
    "for val in flat_mapped:\n",
    "    print(f\"  {val.numpy()}\")\n",
    "\n",
    "print(\"\\nObserva: flat_map expande 1 elemento en MÚLTIPLES elementos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd3edd",
   "metadata": {},
   "source": [
    "## 6. Importancia de Prefetch\n",
    "\n",
    "### 6.1 Comparación de rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f55e793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparando rendimiento...\n",
      "\n",
      "Sin prefetch: 0.014 segundos (20 batches)\n",
      "Con prefetch: 0.016 segundos (20 batches)\n",
      "\n",
      "Mejora con prefetch: -9.1%\n"
     ]
    }
   ],
   "source": [
    "def simulate_training_step(features, labels):\n",
    "    \"\"\"\n",
    "    Simula un paso de entrenamiento que toma tiempo\n",
    "    \"\"\"\n",
    "    time.sleep(0.01)  # Simular procesamiento\n",
    "    return tf.reduce_mean(features)\n",
    "\n",
    "def measure_pipeline_performance(dataset, num_batches=50, name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Mide el tiempo de iteración sobre un dataset\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (features, labels) in enumerate(dataset):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        _ = simulate_training_step(features, labels)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"{name}: {elapsed:.3f} segundos ({num_batches} batches)\")\n",
    "    return elapsed\n",
    "\n",
    "print(\"Comparando rendimiento...\\n\")\n",
    "\n",
    "# Recrear datos para esta comparación (evitar conflictos con variables anteriores)\n",
    "test_features = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [7.0, 8.0]])\n",
    "test_labels = np.array([0, 1, 0, 1])\n",
    "\n",
    "# Dataset SIN prefetch\n",
    "dataset_no_prefetch = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n",
    "dataset_no_prefetch = dataset_no_prefetch.batch(4)\n",
    "\n",
    "# Dataset CON prefetch\n",
    "dataset_with_prefetch = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n",
    "dataset_with_prefetch = dataset_with_prefetch.batch(4)\n",
    "dataset_with_prefetch = dataset_with_prefetch.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "time_no_prefetch = measure_pipeline_performance(dataset_no_prefetch, num_batches=20, name=\"Sin prefetch\")\n",
    "time_with_prefetch = measure_pipeline_performance(dataset_with_prefetch, num_batches=20, name=\"Con prefetch\")\n",
    "\n",
    "improvement = ((time_no_prefetch - time_with_prefetch) / time_no_prefetch) * 100\n",
    "print(f\"\\nMejora con prefetch: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee645651",
   "metadata": {},
   "source": [
    "### 6.2 Visualización del concepto de prefetch\n",
    "\n",
    "**Sin prefetch:**\n",
    "```\n",
    "CPU: [preparar batch 1] -----> [preparar batch 2] -----> [preparar batch 3]\n",
    "GPU:                    [usar batch 1]       [usar batch 2]       [usar batch 3]\n",
    "                        ⬆️ GPU espera        ⬆️ GPU espera        ⬆️ GPU espera\n",
    "```\n",
    "\n",
    "**Con prefetch:**\n",
    "```\n",
    "CPU: [preparar batch 1] [preparar batch 2] [preparar batch 3]\n",
    "GPU:                    [usar batch 1]     [usar batch 2]     [usar batch 3]\n",
    "                        ⬆️ ¡Ya está listo! ⬆️ ¡Ya está listo! ⬆️ ¡Ya está listo!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68d228",
   "metadata": {},
   "source": [
    "## 7. Entrenar un modelo con el dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eb2b2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                64        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 209\n",
      "Trainable params: 209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                64        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 209\n",
      "Trainable params: 209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Crear modelo simple\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(3,)),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bcdd2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo con dataset sharded...\n",
      "\n",
      "Epoch 1/3\n",
      "32/32 [==============================] - 1s 6ms/step - loss: 0.7168 - accuracy: 0.4930\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 1s 6ms/step - loss: 0.7168 - accuracy: 0.4930\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7020 - accuracy: 0.4650\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7020 - accuracy: 0.4650\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6972 - accuracy: 0.4950\n",
      "\n",
      "Entrenamiento completado\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6972 - accuracy: 0.4950\n",
      "\n",
      "Entrenamiento completado\n"
     ]
    }
   ],
   "source": [
    "# Entrenar con dataset sharded\n",
    "print(\"Entrenando modelo con dataset sharded...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    sharded_train_dataset,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenamiento completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864de73",
   "metadata": {},
   "source": [
    "## 8. Best Practices: Pipeline completa recomendada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486bc360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline de producción creada:\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "def create_production_pipeline(\n",
    "    file_pattern,\n",
    "    parse_fn,\n",
    "    batch_size=32,\n",
    "    shuffle_buffer=10000,\n",
    "    is_training=True,\n",
    "    cache=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline de producción siguiendo mejores prácticas\n",
    "    \n",
    "    Orden recomendado:\n",
    "    1. list_files (con shuffle opcional)\n",
    "    2. flat_map o interleave para leer archivos\n",
    "    3. cache (opcional, si dataset cabe en memoria)\n",
    "    4. map con num_parallel_calls\n",
    "    5. shuffle (solo training)\n",
    "    6. repeat (para training continuo)\n",
    "    7. batch\n",
    "    8. prefetch\n",
    "    \"\"\"\n",
    "    # 1. Listar archivos\n",
    "    files = tf.data.Dataset.list_files(file_pattern, shuffle=is_training)\n",
    "    \n",
    "    # 2. Leer archivos en paralelo con interleave (mejor que flat_map)\n",
    "    dataset = files.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=4,  # Leer 4 archivos simultáneamente\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # 3. Cache (opcional)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    \n",
    "    # 4. Parsear con paralelización\n",
    "    dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # 5. Shuffle solo en training\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n",
    "    \n",
    "    # 6. Repeat para training continuo\n",
    "    if is_training:\n",
    "        dataset = dataset.repeat()\n",
    "    \n",
    "    # 7. Batch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    # 8. Prefetch\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Crear pipeline de producción\n",
    "production_dataset = create_production_pipeline(\n",
    "    file_pattern=file_pattern,\n",
    "    parse_fn=parse_csv_line,\n",
    "    batch_size=32,\n",
    "    is_training=True,\n",
    "    cache=False\n",
    ")\n",
    "\n",
    "print(\"Pipeline de producción creada:\")\n",
    "print(production_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00454e7c",
   "metadata": {},
   "source": [
    "## 9. Resumen de Conceptos Clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5de0b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "╔═══════════════════════════════════════════════════════════════════╗\n",
      "║                    RESUMEN DE CONCEPTOS                           ║\n",
      "╠═══════════════════════════════════════════════════════════════════╣\n",
      "║                                                                   ║\n",
      "║ 1. DATASETS EN MEMORIA                                            ║\n",
      "║    • from_tensors()        → TODO como 1 elemento                 ║\n",
      "║    • from_tensor_slices()  → Cada fila = 1 elemento              ║\n",
      "║                                                                   ║\n",
      "║ 2. LEER ARCHIVOS                                                  ║\n",
      "║    • TextLineDataset       → Leer línea por línea                ║\n",
      "║    • list_files             → Encontrar múltiples archivos        ║\n",
      "║                                                                   ║\n",
      "║ 3. TRANSFORMACIONES                                               ║\n",
      "║    • map()                  → 1 entrada → 1 salida                ║\n",
      "║    • flat_map()             → 1 entrada → MUCHAS salidas          ║\n",
      "║    • interleave()           → flat_map + paralelización           ║\n",
      "║                                                                   ║\n",
      "║ 4. OPTIMIZACIONES                                                 ║\n",
      "║    • shuffle()              → Solo en training!                   ║\n",
      "║    • batch()                → Agrupar ejemplos                    ║\n",
      "║    • prefetch()             → CPU/GPU en paralelo                 ║\n",
      "║    • cache()                → Guardar en memoria                  ║\n",
      "║    • num_parallel_calls     → AUTOTUNE para paralelizar           ║\n",
      "║                                                                   ║\n",
      "║ 5. ORDEN RECOMENDADO                                              ║\n",
      "║    list_files → flat_map → cache → map → shuffle → repeat →     ║\n",
      "║    → batch → prefetch                                             ║\n",
      "║                                                                   ║\n",
      "╚═══════════════════════════════════════════════════════════════════╝\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════════╗\n",
    "║                    RESUMEN DE CONCEPTOS                           ║\n",
    "╠═══════════════════════════════════════════════════════════════════╣\n",
    "║                                                                   ║\n",
    "║ 1. DATASETS EN MEMORIA                                            ║\n",
    "║    • from_tensors()        → TODO como 1 elemento                 ║\n",
    "║    • from_tensor_slices()  → Cada fila = 1 elemento              ║\n",
    "║                                                                   ║\n",
    "║ 2. LEER ARCHIVOS                                                  ║\n",
    "║    • TextLineDataset       → Leer línea por línea                ║\n",
    "║    • list_files             → Encontrar múltiples archivos        ║\n",
    "║                                                                   ║\n",
    "║ 3. TRANSFORMACIONES                                               ║\n",
    "║    • map()                  → 1 entrada → 1 salida                ║\n",
    "║    • flat_map()             → 1 entrada → MUCHAS salidas          ║\n",
    "║    • interleave()           → flat_map + paralelización           ║\n",
    "║                                                                   ║\n",
    "║ 4. OPTIMIZACIONES                                                 ║\n",
    "║    • shuffle()              → Solo en training!                   ║\n",
    "║    • batch()                → Agrupar ejemplos                    ║\n",
    "║    • prefetch()             → CPU/GPU en paralelo                 ║\n",
    "║    • cache()                → Guardar en memoria                  ║\n",
    "║    • num_parallel_calls     → AUTOTUNE para paralelizar           ║\n",
    "║                                                                   ║\n",
    "║ 5. ORDEN RECOMENDADO                                              ║\n",
    "║    list_files → flat_map → cache → map → shuffle → repeat →     ║\n",
    "║    → batch → prefetch                                             ║\n",
    "║                                                                   ║\n",
    "╚═══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae85449",
   "metadata": {},
   "source": [
    "## 10. Cleanup - Limpiar archivos temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3743c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(TEMP_DIR):\n",
    "    shutil.rmtree(TEMP_DIR)\n",
    "    print(f\"Directorio temporal eliminado: {TEMP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5560d2",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Este notebook cubrió:\n",
    "\n",
    "1. **Diferencia entre `from_tensors()` y `from_tensor_slices()`**\n",
    "2. **Lectura de archivos CSV con `TextLineDataset`**\n",
    "3. **Parseo de datos con `map()` y `tf.io.decode_csv()`**\n",
    "4. **Manejo de datasets sharded con `flat_map()` e `interleave()`**\n",
    "5. **Diferencia entre `map()` y `flat_map()`**\n",
    "6. **Importancia de `prefetch()` para rendimiento**\n",
    "7. **Pipeline de producción completa**\n",
    "8. **Best practices: orden recomendado de operaciones**\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- `from_tensor_slices()` es el método principal para crear datasets\n",
    "- `TextLineDataset` lee archivos sin cargar todo en memoria\n",
    "- `flat_map()` expande 1 elemento en múltiples (archivos → líneas)\n",
    "- `shuffle()` solo se aplica en training, nunca en validación/test\n",
    "- `prefetch()` es esencial para mantener GPU ocupada\n",
    "- El orden de operaciones importa para el rendimiento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
