{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1394e2fe",
   "metadata": {},
   "source": [
    "# Redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2629bc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a01981",
   "metadata": {},
   "source": [
    "## Conceptos\n",
    "\n",
    "| Concepto      | Qué significa                             |\n",
    "| ------------- | ----------------------------------------- |\n",
    "| Forward pass  | Calcular predicción usando pesos actuales |\n",
    "| Loss          | Cuánto se equivoca el modelo              |\n",
    "| Backward pass | Calcula derivadas con `GradientTape`      |\n",
    "| Gradient      | Dirección de corrección para cada peso    |\n",
    "| Update rule   | `w = w - learning_rate * gradient`        |\n",
    "| Objetivo      | Minimizar la pérdida                      |\n",
    "\n",
    "- Recordatorio del flujo\n",
    "\n",
    "| Paso | Nombre         | Qué pasa                                                       |\n",
    "| ---- | -------------- | -------------------------------------------------------------- |\n",
    "| 1  | Forward pass   | La red hace predicción con los pesos actuales                  |\n",
    "| 2  | Compute loss   | Comparas predicción vs valor real                              |\n",
    "| 3  | Backward pass  | Calculas gradientes (derivadas) con `GradientTape`             |\n",
    "| 4  | Update weights | Ajustas pesos usando gradientes (SGD o regla de actualización) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae525c",
   "metadata": {},
   "source": [
    "## Ejemplo 1 — Una neurona simple (y = wx + b)\n",
    "\n",
    "- Objetivo: aprender el valor correcto de w y b para aproximar la función:\n",
    "    - y = 2x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e68201f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrada (x): [1. 2. 3. 4.]\n",
      "Datos reales (y_real): [3. 5. 7. 9.]\n"
     ]
    }
   ],
   "source": [
    "# Datos\n",
    "x = tf.constant([1.0, 2.0, 3.0, 4.0])\n",
    "y_real = tf.constant([3.0, 5.0, 7.0, 9.0])  # 2x + 1\n",
    "\n",
    "print(\"Datos de entrada (x):\", x.numpy())\n",
    "print(\"Datos reales (y_real):\", y_real.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6de4831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos iniciales: w = [1.3552873] , b = [0.00364656]\n",
      "Learning rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Pesos iniciales (random)\n",
    "w = tf.Variable(tf.random.normal([1], dtype=tf.float32))\n",
    "b = tf.Variable(tf.random.normal([1], dtype=tf.float32))\n",
    "\n",
    "lr = 0.1  # Learning rate\n",
    "\n",
    "print(\"Pesos iniciales: w =\", w.numpy(), \", b =\", b.numpy())\n",
    "print(\"Learning rate:\", lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c94265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss=7.3219  w=[2.8205333]  b=[0.5252736]\n",
      "Epoch 10  Loss=0.0465  w=[2.1803463]  b=[0.50511414]\n",
      "Epoch 20  Loss=0.0240  w=[2.125319]  b=[0.63219166]\n",
      "Epoch 30  Loss=0.0131  w=[2.09233]  b=[0.72855055]\n",
      "Epoch 40  Loss=0.0071  w=[2.0681264]  b=[0.79969984]\n",
      "Epoch 50  Loss=0.0039  w=[2.0502696]  b=[0.8522009]\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "\n",
    "# Número de épocas\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass, calcular predicción\n",
    "        y_pred = w * x + b\n",
    "\n",
    "        # Loss: MSE, calcular pérdida\n",
    "        loss = tf.reduce_mean((y_real - y_pred) ** 2)\n",
    "\n",
    "    # Backward pass, calcular gradientes\n",
    "    # En qué dirección y cuánto deben cambiar w y b para disminuir la pérdida.\n",
    "    # Matemáticamente son derivadas parciales\n",
    "    # Son como señales de corrección\n",
    "    dw, db = tape.gradient(loss, [w, b])\n",
    "\n",
    "    # Update, actualizar pesos de acuerdo a los gradientes\n",
    "    # lr = learning rate: qué tan rápido aprende\n",
    "    w.assign_sub(lr * dw)\n",
    "    b.assign_sub(lr * db)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}  Loss={loss.numpy():.4f}  w={w.numpy()}  b={b.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b9c45",
   "metadata": {},
   "source": [
    "- Resumen del flujo\n",
    "\n",
    "| Paso           | Qué hace                            | Metáfora                                   |\n",
    "| -------------- | ----------------------------------- | ------------------------------------------ |\n",
    "| 1 Forward    | Predice algo con `w` y `b` actuales | El estudiante responde una pregunta        |\n",
    "| 2 Loss       | Mide qué tan mal respondió          | Le dices qué tan equivocado estuvo         |\n",
    "| 3 Gradientes | Calcula cómo corregir               | Le dices **cómo mejorar**, no la respuesta |\n",
    "| 4 Update     | Cambia `w` y `b`                    | Aprende y ajusta su conocimiento           |\n",
    "\n",
    "- Resumen:\n",
    "    - w y b: lo que el modelo sabe (parametros entrenables)\n",
    "    - y_pred = w * x + b: predicción actual\n",
    "    - loss: qué tanto se equivocó\n",
    "    - dw, db: cómo debe cambiar\n",
    "    - assign_sub: aplica el aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c265515",
   "metadata": {},
   "source": [
    "- Metáfora rápida. Imagina que quieres lanzar una flecha a un blanco con los ojos vendados.\n",
    "    - Primer tiro → cae lejos (loss alto)\n",
    "    - Te dicen: \"más arriba, más derecha\" (gradiente)\n",
    "    - Ajustas el tiro (updates)\n",
    "    - Repetición → mejora progresiva (epochs)\n",
    "    - Al final → tiro perfecto (loss casi cero)\n",
    "- Eso es gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d975d",
   "metadata": {},
   "source": [
    "## Ejemplo 2 — Neurona con activación (sigmoid)\n",
    "\n",
    "- Ahora hacemos una neurona que aprenda una puerta lógica AND:\n",
    "\n",
    "| x1 | x2 | y |\n",
    "| -- | -- | - |\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 0 |\n",
    "| 1  | 0  | 0 |\n",
    "| 1  | 1  | 1 |\n",
    "\n",
    "- Una función de activación es la transformación aplicada al valor z para convertirlo en una salida útil.\n",
    "- Con activación, la red puede aprender:\n",
    "    - límites no lineales\n",
    "    - clasificaciones\n",
    "    - patrones complejos\n",
    "    - lógica (como AND, OR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f430ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset (X):\n",
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "Etiquetas reales (y_true):\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "\n",
      "Pesos iniciales para clasificación:\n",
      " w = [[ 1.0883665 ]\n",
      " [-0.92451656]] , b = [0.]\n",
      "Learning rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "X = tf.constant([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
    "y_true = tf.constant([[0.],[0.],[0.],[1.]])\n",
    "\n",
    "print(\"\\nDataset (X):\")\n",
    "print(X.numpy())\n",
    "print(\"Etiquetas reales (y_true):\")\n",
    "print(y_true.numpy())\n",
    "\n",
    "# Pesos\n",
    "w = tf.Variable(tf.random.normal([2,1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "print(\"\\nPesos iniciales para clasificación:\\n w =\", w.numpy(), \", b =\", b.numpy())\n",
    "print(\"Learning rate:\", lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "406121d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss=0.75512\n",
      " w = [[ 1.0811429]\n",
      " [-0.9201393]] , b = [-0.02682458]\n",
      "Epoch 400 Loss=0.25765\n",
      " w = [[1.9059958]\n",
      " [1.6442373]] , b = [-2.9538276]\n",
      "Epoch 800 Loss=0.16732\n",
      " w = [[2.7332985]\n",
      " [2.680386 ]] , b = [-4.2896504]\n",
      "Epoch 1200 Loss=0.12416\n",
      " w = [[3.3561697]\n",
      " [3.3413727]] , b = [-5.2316513]\n",
      "Epoch 1600 Loss=0.09843\n",
      " w = [[3.845845]\n",
      " [3.840657]] , b = [-5.962949]\n",
      "\n",
      "Predicciones finales:\n",
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward\n",
    "        z = tf.matmul(X, w) + b\n",
    "        y_pred = tf.sigmoid(z)\n",
    "\n",
    "        # Loss binary crossentropy\n",
    "        loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true, y_pred))\n",
    "\n",
    "    # Backpropagation\n",
    "    grads = tape.gradient(loss, [w, b])\n",
    "\n",
    "    # Update\n",
    "    w.assign_sub(lr * grads[0])\n",
    "    b.assign_sub(lr * grads[1])\n",
    "\n",
    "    if epoch % 400 == 0:\n",
    "        print(f\"Epoch {epoch} Loss={loss.numpy():.5f}\")\n",
    "        print(\" w =\", w.numpy(), \", b =\", b.numpy())\n",
    "\n",
    "print(\"\\nPredicciones finales:\")\n",
    "print(tf.round(tf.sigmoid(tf.matmul(X, w) + b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c562b",
   "metadata": {},
   "source": [
    "### Glosario Esencial de Redes Neuronales\n",
    "\n",
    "### 1. Entrada (Input)\n",
    "\n",
    "Datos que alimentan la red.\n",
    "Puede ser:\n",
    "\n",
    "* Un número (ej: temperatura)\n",
    "* Un vector (ej: pixeles de un dígito)\n",
    "* Una matriz (imagen en escala de grises)\n",
    "* Un tensor (imagen con canales o batch)\n",
    "\n",
    "Ejemplo:\n",
    "Para reconocer si una imagen contiene un gato, los valores de los píxeles son la entrada.\n",
    "\n",
    "### 2. Neurona (Nodo)\n",
    "\n",
    "Es la unidad básica de cálculo.\n",
    "Toma entradas, las multiplica por pesos, suma el bias, aplica una activación y produce una salida.\n",
    "\n",
    "La fórmula base es:\n",
    "\n",
    "```bash\n",
    "z = w1*x1 + w2*x2 + ... + wn*xn + b\n",
    "y = activation(z)\n",
    "```\n",
    "\n",
    "Igual que una neurona biológica: recibe señales, las procesa y dispara si son suficientes.\n",
    "\n",
    "### 3. Pesos (Weights, w)\n",
    "\n",
    "Son los valores que dicen qué tan importante es cada entrada.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "* Si `w1 = 0`, la entrada x1 no importa.\n",
    "* Si `w2 = 100`, esa entrada domina la decisión.\n",
    "\n",
    "Los pesos son el conocimiento aprendido.\n",
    "\n",
    "### 4. Bias (b)\n",
    "\n",
    "Número libre que permite desplazar la salida antes de aplicar la activación.\n",
    "Actúa como un umbral: decide cuánta evidencia hace falta para activar la neurona.\n",
    "\n",
    "Sin bias, todas las decisiones pasarían por el punto (0,0) → menos flexible.\n",
    "\n",
    "### 5. Función de Activación\n",
    "\n",
    "Transforma el valor `z` en algo útil.\n",
    "Convierte una suma lineal en una decisión no lineal, permitiendo aprender patrones complejos.\n",
    "\n",
    "Ejemplos:\n",
    "\n",
    "| Activación | Uso                   | Ejemplo salida                       |\n",
    "| ---------- | --------------------- | ------------------------------------ |\n",
    "| Sigmoid    | Clasificación binaria | 0.01 → 0.99                          |\n",
    "| ReLU       | Redes profundas       | max(0, z)                            |\n",
    "| Softmax    | Multiclase            | probabilidades (clase 0.2, 0.5, 0.3) |\n",
    "\n",
    "Sin activación, una red neuronal sería solo una regresión lineal.\n",
    "\n",
    "### 6. Capa (Layer)\n",
    "\n",
    "Una colección de neuronas trabajando juntas.\n",
    "\n",
    "Tipos comunes:\n",
    "\n",
    "* Capa densa (Fully Connected): cada neurona conecta con todas las entradas.\n",
    "* Convolucional (CNN): detecta patrones espaciales (imágenes).\n",
    "* Recurrente (RNN, LSTM, GRU): maneja secuencias (texto, audio).\n",
    "* Transformers: atención, NLP moderno (ChatGPT, BERT).\n",
    "\n",
    "### 7. Forward Pass (Propagación hacia adelante)\n",
    "\n",
    "El proceso donde la red toma los datos y genera una predicción usando los pesos actuales.\n",
    "\n",
    "Es como un estudiante respondiendo un examen sin mirar la respuesta aún.\n",
    "\n",
    "### 8. Función de Pérdida (Loss / Cost Function)\n",
    "\n",
    "Mide qué tan mal predijo la red comparado con el objetivo real.\n",
    "\n",
    "Ejemplos:\n",
    "\n",
    "* MSE (Regresión): `(y_true - y_pred)²`\n",
    "* Binary Crossentropy (Binario)\n",
    "* Categorical Crossentropy (Multiclase)\n",
    "\n",
    "Si el loss baja → la red está aprendiendo.\n",
    "\n",
    "### 9. Gradient (Gradiente)\n",
    "\n",
    "La derivada de la pérdida respecto a los pesos.\n",
    "\n",
    "Dicen:\n",
    "\n",
    "> \"Si ajustas este peso en sentido positivo o negativo, ¿la pérdida mejora?\"\n",
    "\n",
    "Son instrucciones, no los pesos finales.\n",
    "\n",
    "### 10. Backpropagation\n",
    "\n",
    "El algoritmo que usa los gradientes para ajustar los pesos calculando derivadas desde la salida hacia atrás.\n",
    "\n",
    "\"Aprender de los errores.\"\n",
    "\n",
    "### 11. Optimizador (Optimizer)\n",
    "\n",
    "Método matemático que usa los gradientes para actualizar los pesos.\n",
    "\n",
    "Ejemplos:\n",
    "\n",
    "| Optimizador | Característica                |\n",
    "| ----------- | ----------------------------- |\n",
    "| SGD         | Simple, lento                 |\n",
    "| Momentum    | Más estable                   |\n",
    "| Adam        | Más usado, rápido, adaptativo |\n",
    "| RMSProp     | Bueno para secuencias         |\n",
    "\n",
    "Decide cómo corregir los pesos.\n",
    "\n",
    "### 12. Learning Rate (LR)\n",
    "\n",
    "Controla qué tan grande es cada paso de actualización.\n",
    "\n",
    "* Muy bajo → aprendizaje lento.\n",
    "* Muy alto → aprendizaje inestable.\n",
    "\n",
    "Es la “velocidad de aprendizaje”.\n",
    "\n",
    "### 13. Epoch\n",
    "\n",
    "Una pasada completa sobre el dataset durante entrenamiento.\n",
    "\n",
    "Si tienes 100 ejemplos y batch size 10 → una época tiene 10 actualizaciones.\n",
    "\n",
    "### 14. Batch y Batch Size\n",
    "\n",
    "* Batch: subconjunto de datos usados en una actualización.\n",
    "* Batch size: tamaño de ese subconjunto.\n",
    "\n",
    "Entrenar por batches evita usar toda la memoria y hace más eficiente el aprendizaje.\n",
    "\n",
    "### 15. Predicción / Output\n",
    "\n",
    "El resultado final que produce la red.\n",
    "\n",
    "* Puede ser un número (regresión)\n",
    "* Probabilidad (clasificación)\n",
    "* Clase (argmax)\n",
    "\n",
    "### MINI RESUMEN TABLA\n",
    "\n",
    "| Elemento     | Rol                         |\n",
    "| ------------ | --------------------------- |\n",
    "| Pesos        | Aprendizaje principal       |\n",
    "| Bias         | Ajusta el umbral            |\n",
    "| Activación   | Hace no lineal la función   |\n",
    "| Forward pass | Predice                     |\n",
    "| Loss         | Evalúa qué tan mal          |\n",
    "| Gradientes   | Señalan corrección          |\n",
    "| Optimizador  | Ajusta pesos                |\n",
    "| Epoch        | Iteración sobre dataset     |\n",
    "| Batch        | División para entrenamiento |\n",
    "\n",
    "### En una frase:\n",
    "\n",
    "> Una red neuronal es un conjunto de neuronas conectadas, con pesos y bias ajustados mediante gradientes durante muchos epochs usando una función de pérdida y un optimizador, donde las activaciones permiten aprender patrones complejos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e6883e",
   "metadata": {},
   "source": [
    "## Ejemplo Final: Clasificador binario de puntos en 2D\n",
    "\n",
    "- Queremos que la red clasifique puntos en:\n",
    "    - Clase 0 → izquierda de la línea\n",
    "    - Clase 1 → derecha de la línea\n",
    "\n",
    "| x1  | x2  | Clase |\n",
    "| --- | --- | ----- |\n",
    "| 1.0 | 1.0 | 0     |\n",
    "| 2.0 | 1.0 | 0     |\n",
    "| 3.0 | 2.0 | 0     |\n",
    "| 4.0 | 5.0 | 1     |\n",
    "| 5.0 | 7.0 | 1     |\n",
    "| 6.0 | 9.0 | 1     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe4d392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset (X):\n",
      "[[1. 1.]\n",
      " [2. 1.]\n",
      " [3. 2.]\n",
      " [4. 5.]\n",
      " [5. 7.]\n",
      " [6. 9.]]\n",
      "Etiquetas reales (y_true):\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# 1) DATASET\n",
    "X = tf.constant([\n",
    "    [1.,1.],\n",
    "    [2.,1.],\n",
    "    [3.,2.],\n",
    "    [4.,5.],\n",
    "    [5.,7.],\n",
    "    [6.,9.]\n",
    "])\n",
    "\n",
    "y_true = tf.constant([\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [1.],\n",
    "    [1.],\n",
    "    [1.]\n",
    "])\n",
    "\n",
    "print(\"Dataset (X):\")\n",
    "print(X.numpy())\n",
    "print(\"Etiquetas reales (y_true):\")\n",
    "print(y_true.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1962e1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos iniciales:\n",
      " W = [[-0.04149994]\n",
      " [ 1.2491224 ]] , b = [0.]\n",
      "Learning rate: 0.05\n"
     ]
    }
   ],
   "source": [
    "# 2) MODEL PARAMETERS (learnable)\n",
    "W = tf.Variable(tf.random.normal([2, 1]))  # weights\n",
    "b = tf.Variable(tf.zeros([1]))             # bias\n",
    "\n",
    "# 3) HYPERPARAMETERS\n",
    "lr = 0.05      # learning rate\n",
    "epochs = 2000\n",
    "\n",
    "print(\"\\nPesos iniciales:\\n W =\", W.numpy(), \", b =\", b.numpy())\n",
    "print(\"Learning rate:\", lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a6e642c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss=0.8953\n",
      "Weights: [[-0.08340788]\n",
      " [ 1.2212142 ]] \n",
      "Bias: [-0.02037188] \n",
      "\n",
      "Epoch 400, Loss=0.0954\n",
      "Weights: [[-1.4385846]\n",
      " [ 1.8880917]] \n",
      "Bias: [-1.6622312] \n",
      "\n",
      "Epoch 800, Loss=0.0561\n",
      "Weights: [[-1.7291387]\n",
      " [ 2.3555784]] \n",
      "Bias: [-2.3420618] \n",
      "\n",
      "Epoch 1200, Loss=0.0398\n",
      "Weights: [[-1.9013381]\n",
      " [ 2.651195 ]] \n",
      "Bias: [-2.7959914] \n",
      "\n",
      "Epoch 1600, Loss=0.0308\n",
      "Weights: [[-2.022794 ]\n",
      " [ 2.8673017]] \n",
      "Bias: [-3.138076] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        z = tf.matmul(X, W) + b\n",
    "        y_pred = tf.sigmoid(z)\n",
    "        \n",
    "        # Loss (binary crossentropy)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true, y_pred))\n",
    "    \n",
    "    # Backpropagation\n",
    "    gradients = tape.gradient(loss, [W, b])\n",
    "    \n",
    "    # Update\n",
    "    W.assign_sub(lr * gradients[0])\n",
    "    b.assign_sub(lr * gradients[1])\n",
    "    \n",
    "    # Monitor progress occasionally\n",
    "    if epoch % 400 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss={loss.numpy():.4f}\")\n",
    "        print(\"Weights:\", W.numpy(), \"\\nBias:\", b.numpy(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d590095c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Predictions:\n",
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], shape=(6, 1), dtype=float32)\n",
      "\n",
      "Final Weights: [[-2.1160235]\n",
      " [ 3.0370913]]\n",
      "Final Bias: [-3.41217]\n"
     ]
    }
   ],
   "source": [
    "# FINAL PREDICTIONS\n",
    "print(\"\\n Final Predictions:\")\n",
    "print(tf.round(tf.sigmoid(tf.matmul(X, W) + b)))\n",
    "print(\"\\nFinal Weights:\", W.numpy())\n",
    "print(\"Final Bias:\", b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e7ca1",
   "metadata": {},
   "source": [
    "- Interpretación de resultados\n",
    "\n",
    "| Concepto                   | Lo que sucedió                                         |\n",
    "| -------------------------- | ------------------------------------------------------ |\n",
    "| **Forward pass**           | La red hacía predicciones usando `W` y `b`.            |\n",
    "| **Loss**                   | Medía qué tan lejos estaba de la respuesta real.       |\n",
    "| **Gradients**              | La red calculó en qué dirección corregir los pesos.    |\n",
    "| **Optimizer (SGD manual)** | Usó `assign_sub()` para ajustar los parámetros.        |\n",
    "| **Sigmoid**                | Convirtió `z` en probabilidades entre 0 y 1.           |\n",
    "| **Epochs**                 | Repeticiones del ciclo mejora→evaluación.              |\n",
    "| **Resultado final**        | El modelo aprendió a separar las clases correctamente. |\n",
    "\n",
    "- Significado a nivel conceptual\n",
    "\n",
    "| Componente     | Rol aprendido                                                                              |\n",
    "| -------------- | ------------------------------------------------------------------------------------------ |\n",
    "| `W[0] = -1.73` | Le dice a la red que cuando x1 aumenta, la probabilidad debería bajar                      |\n",
    "| `W[1] = +2.79` | Le dice que x2 tiene más influencia en la clasificación                                    |\n",
    "| `b = -3.79`    | Exige que la combinación de entradas sea suficientemente grande antes de activar la salida |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
